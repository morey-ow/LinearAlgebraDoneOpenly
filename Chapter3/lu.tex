
\begin{center} 
\emph{``Whenever you're in conflict with someone, there is one factor that can make the difference between damaging your relationship and deepening it. That factor is attitude.'' -- William James}
\end{center}

\section{Matrix Factorizations}\label{sec:lu}
In algebra, it is often important to be able to undo the process of multiplication, which is known as factorization. We see it in arithmetic, such as $6 = 2\cdot 3$, and we see it in polynomials, $x^2-1 = (x-1)(x+1)$. Factorization can be extremely useful in solving problems where such objects arise, for example, when trying to solve the polynomial equation \[x^2-1=0,\] the subsequent factorization \[(x-1)(x+1) = 0\] illuminates the solution set $\{1, -1\}$. Such factorizations for matrices can be equally useful. Although no equivalent of prime numbers or irreducible polynomials exist for matrices, there are many very useful factorizations for matrices, much like the elementary factorizations we saw in the previous section. In this section, we discuss generalizations of the elementary matrices from the previous section and discussion their presence in matrix factorizations, in particular the $LU$ factorization.\\

\subsection{Generalizations of Elementary Matrices}
\begin{Def}\label{def:diagonal}  A \textbf{diagonal matrix} is a square $n\times n$ matrix whose nondiagonal entries are all zero.
\end{Def}

%Adym Warhurst
\begin{center}
\begin{tikzpicture}
\path (0,0) node {$D = \mtx{rrrr}{ d_1 & 0& \cdots & 0 \\ 0 &d_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_n},$};
\draw[ultra thick, red] (-1.25,1) -- (1.25,-1.5);
\begin{scope}[shift={(0.45,0.45)}]
\draw[ultra thick, red] (-1.25,1) -- (1.25,-1.5);
\end{scope}

\path (5,0) node {$D^{-1} = \mtx{rrrr}{ \frac{1}{d_1} & 0& \cdots & 0 \\ 0 & \frac{1}{d_2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac{1}{d_n}}.$};
\begin{scope}[shift={(5.15,0)}]
\draw[ultra thick, red] (-1.25,1) -- (1.25,-1.5);
\begin{scope}[shift={(0.45,0.45)}]
\draw[ultra thick, red] (-1.25,1) -- (1.25,-1.5);
\end{scope}
\end{scope}
\end{tikzpicture}
\end{center}

The identity matrix $I_n$ is a diagonal matrix all whose diagonal entries are 1. The zero matrix is a diagonal matrix all whose diagonal entries are 0. In a diagonal matrix, the diagonal entries need not be the same.
In general, an $n\times n$ diagonal matrix $D$ is of the form displayed to the right. Every diagonal matrix can be factored as a product of elementary matrices of scaling type (if we allow the possibility of zero scaling). As scaling type  elementary matrices can have at most one non-unital diagonal entry, we can view diagonal matrices as their generalization. Similar to how scaling elementary matrices multiply, we see that if $A$ is a matrix, then $DA$ is the matrix where the $i$th row of $A$ is scaled by $d_i$. Likewise, $AD$ is the matrix where the $j$th column of $A$ is scaled by $d_j$. In particular, a product of two diagonal matrices is a diagonal matrix whose diagonal entries are the products of the corresponding diagonal entries.\\

Furthermore, if no diagonal entry is zero, a diagonal matrix $D$ is a product of elementary matrices and hence is  invertible by %(b) of 
the Nonsingular Matrix Theorem. The sum and scalar multiples of diagonal matrices are diagonal matrices. The set of diagonal matrices forms an $n$-dimensional subspace of $F^{n\times n}$. Additionally, the product of two diagonal matrices is diagonal. 


\begin{Exam} Let $A = \mtx{rrr}{-\frac{1}{2} & 0 & 0\\ 0 & 1 & 0 \\0& 0& 3}$. Then $A$ is a diagonal matrix and
\[A^{-1} =\mtx{rrr}{-2 & 0 & 0\\ 0 & 1& 0 \\0& 0& \frac{1}{3}}, \quad A^4 = \mtx{rrr}{\frac{1}{16} & 0 & 0\\ 0 & 1& 0 \\0& 0& 81}, \quad A^{-4} = \mtx{rrr}{16 & 0 & 0\\ 0 & 1& 0 \\0& 0& \frac{1}{81}}.\] Furthermore, $A$ can be factored into a product of scaling elementary matrices:
\[A = \mtx{rrr}{-\frac{1}{2}&0&0\\0&1&0\\0&0&1}\mtx{rrr}{1&0&0\\0&1&0\\0&0&3}.\qedhere\]
\end{Exam}\vs

\begin{Def} A diagonal matrix $C$ is called a \textbf{scalar matrix} if all the diagonal entries are equal.
\end{Def}\vs

Essentially, scalar matrices all have the form $C = cI_n$ for some scalar $c$, which is why they get their name. Note that for any matrix $A$, we have that $CA = (cI_n)A = c(I_nA) = cA$, that is, multiplication by a scalar matrix is no different than scalar multiplication. Furthermore, scalar matrices are exactly those matrices which commute with all other matrices with regard to multiplication.\\


\begin{Def} A \textbf{permutation matrix} is a square matrix formed by some rearrangement of the rows of $I_n$.\end{Def}\vs

It can be shown that any permutation of objects can be accomplished by a sequence of transpositions, that is, a rearrangement of only two things at a time. This implies that a permutation matrix is a product of elementary matrices of interchange type. As such, a permutation matrix $P$ is always non-singular. In fact, $P^{-1}=P^\top $. As an interchange elementary matrix has at most two rows permuted, we can view permutation matrices as their generalization. Similar to how interchange elementary matrices multiply, we see that if $A$ is a matrix, then $PA$ is the matrix where the rows of $A$ are rearranged in the same way that the rows in $I_n$ are rearranged for $P$.  Likewise, $AP$ is the matrix where the columns of $A$ are rearranged in the same way that the columns in $I_n$ are rearranged for $P$. In particular, a product of two permutation matrices is a permutation matrix, although the sum and scalar multiples of permutation matrices are no longer permutation matrices.\\

\begin{Exam} Let $P = \mtx{rrrr}{0&1&0&0\\0&0&0&1\\1&0&0&0\\0&0&1&0}$ is a permutation matrix. It can be easily verified that $P^\top =P^{-1}$. We also see that $P$ factors as a product of interchange elementary matrices:
\[P = \mtx{rrrr}{0&0&1&0\\0&1&0&0\\1&0&0&0\\0&0&0&1}\mtx{rrrr}{1&0&0&0\\0&0&1&0\\0&1&0&0\\0&0&0&1}\mtx{rrrr}{1&0&0&0\\0&1&0&0\\0&0&0&1\\0&0&1&0}\qedhere\]
\end{Exam}\vs

\begin{Def}  An \textbf{upper triangular matrix} is a square $n\times n$ matrix whose entries below the main diagonal are all zero. A \textbf{lower triangular matrix} is a square $n\times n$ matrix whose entries above the main diagonal are all zero. A triangular that is either upper or lower triangular is called a \textbf{triangular matrix}. A matrix is \textbf{unit triangular} if it is triangular and all the diagonal entries are 1. A matrix is \textbf{strictly triangular} if it is triangular and all the diagonal entries are 0.
\end{Def}\vs

%Adym Warhurst
\begin{center}
\begin{tikzpicture}
\path (0,0) node {$\mtx{rrr}{1&3&-1\\0&-3&2\\0&0&2}$};
\path (0,1.5) node {Upper Triangular Matrix};
\draw[ultra thick, red] (-1.5,1) -- (1,-1.5) -- (1,1) -- cycle;

\begin{scope}[shift={(5,0)}]
\path (0,0) node {$\mtx{rrr}{\color{blue}1&5&3\\0&\color{blue}1&2\\0&0&\color{blue}1}$};
\path (0,1.5) node {Unit Upper Triangular Matrix};
\draw[ultra thick, red] (-1.25,1) -- (0.75,-1.25) -- (0.75,1) -- cycle;
\begin{scope}[shift={(0.35,0.35)}]
\clip (-0.9,-0.85) rectangle (1,1);
\draw[ultra thick, red!50, dashed] (-1.25,1) -- (0.75,-1.25);
\end{scope}
\end{scope}

\begin{scope}[shift={(11,0)}]
\path (0,0) node {$\mtx{rrr}{\color{blue}0&7&-3\\0&\color{blue}0&-1\\0&0&\color{blue}0\ }$};
\path (0,1.5) node {Strictly Upper Triangular Matrix};
\draw[ultra thick, red] (-1.3,1) -- (0.8,-1.3) -- (0.85,1) -- cycle;
\begin{scope}[shift={(0.3,0.3)}]
\clip (-0.9,-0.85) rectangle (1,1);
\draw[ultra thick, red!50, dashed] (-1.3,1) -- (0.8,-1.3);
\end{scope}
\end{scope}


\begin{scope}[shift={(0,-4)}]
\path (0,0) node {$\mtx{rrr}{1&0&0\\3&-3&0\\-2&1&0}$};
\path (0,1.75) node {Lower Triangular Matrix};
\draw[ultra thick, red] (-1,1.5) -- (1.6,-1.15) -- (-1,-1.15) -- cycle;

\begin{scope}[shift={(5,0)}]
\path (0,0) node {$\mtx{rrr}{\color{blue}1&0&0\\-3&\color{blue}1&0\\5&-7&\color{blue}1}$};
\path (0,1.75) node {Unit Lower Triangular Matrix};
\draw[ultra thick, red] (-1,1.5) -- (1.6,-1.15) -- (-1,-1.15) -- cycle;
\begin{scope}[shift={(-0.35,-0.35)}]
\clip (-1.25,-1) rectangle (1,1.15);
\draw[ultra thick, red!50, dashed] (-1,1.5) -- (1.6,-1.15);
\end{scope}
\end{scope}

\begin{scope}[shift={(11,0)}]
\path (0,0) node {$\mtx{rrr}{\color{blue}0&0&0\\-1&\color{blue}0&0\\1&2&\color{blue} \ 0}$};
\path (0,1.75) node {Strictly Lower Triangular Matrix};
\draw[ultra thick, red] (-0.9,1.5) -- (1.5,-1.15) -- (-0.9,-1.15) -- cycle;
\begin{scope}[shift={(-0.35,-0.35)}]
\clip (-1.25,-1) rectangle (1,1.15);
\draw[ultra thick, red!50, dashed] (-0.9,1.5) -- (1.5,-1.15);
\end{scope}
\end{scope}

\end{scope}
\end{tikzpicture}
\end{center}

In other words, an upper triangular matrix is a square matrix in echelon form, and a lower triangular matrix is just a square matrix in upside-down echelon form\footnotemark[2]. Of course, if a matrix is upper AND lower triangular, then it is actually a diagonal matrix.\\

An upper unit triangular matrix is a product of elementary matrices of replacement type, the kind used during the back-phase of Gauss--Jordan elimination and hence have non-zero entries above the diagonal. In general, an upper triangular matrix is a product of ``backward'' replacement elementary matrices and scaling (including by zero) elementary matrices. In particular, unit upper triangular matrices are those which are strictly a product of ``backward'' replacement elementary matrices, and hence can be viewed as their generalization. Upper triangular matrices are closed under sums and scalars, forming an $\dfrac{n(n-1)}{2}$-dimensional subspace of $F^{n\times n}$. Additionally, a product of upper triangular matrices is upper triangular, and an upper triangular matrix is invertible if and only if none of the diagonal entries are zero, in which case the inverse is likewise upper triangular. Furthermore, the transpose of an upper triangular matrix is lower triangular. Similar statements can be made about lower triangular matrices.\\

\begin{Exam} Let $A = \mtx{rrr}{2&0&-5\\0&1&-3\\0&0&4}$ and $B = \mtx{rrr}{3&-1&-2\\0&0&-5\\0&0&3}$. Then $A$ is nonsingular, but $B$ is not by considering their diagonal entries. In particular,
\[A^{-1} = \mtx{rrr}{\frac{1}{2}&0&\frac{5}{8}\\0&1&\frac{3}{4}\\0&0&\frac{1}{4}},\quad AB = \mtx{rrr}{6&-2&-19\\0&0&-14\\0&0&12}.\] We also see that $A$ factors as a product of replacement and scaling elementary matrices (in the case of $B$ we allow for a zero-scaling matrix, which technically is not an elementary matrix):
\begin{multline*} A = \mtx{rrr}{1&0&0\\0&1&0\\0&0&4}\mtx{rrr}{1&0&-5\\0&1&0\\0&0&1}\mtx{rrr}{1&0&0\\0&1&-3\\0&0&1}\mtx{rrr}{2&0&0\\0&1&0\\0&0&1}\quad\text{and}\quad\\ B = \mtx{rrr}{1&0&0\\0&1&0\\0&0&3}\mtx{rrr}{1&0&-2\\0&1&0\\0&0&1}\mtx{rrr}{1&0&0\\0&1&-5\\0&0&1}\mtx{rrr}{1&0&0\\0&0&0\\0&0&1}\mtx{rrr}{1&-1&0\\0&1&0\\0&0&1} \mtx{rrr}{3&0&0\\0&1&0\\0&0&1}.\qedhere\end{multline*}
\end{Exam}\vs

\subsection{The $LU$ Factorization}
\begin{Thm} Let $A$ be an $m\times n$ matrix which has an echelon form which can be obtained solely by replacement row operations. Then there exists an $m\times n$ matrix $U$ and an $m\times m$ matrix $L$ such that 
\[A = LU,\] $U$ is an echelon form of $A$  and $L$ is a lower unit triangular matrix. In particular, the $(i,j)$-position of $L$ for $i> j$ is $-c$ when the replacement $\row i\mapsto \row i + c\row j$ was used during the row reduction process. 
\end{Thm}
\begin{proof}
When computing an echelon form of a matrix, the scaling row operation is never necessary. This one is only necessary for row-reduced echelon form. Thus, we need only use replacement or interchange to row reduce a matrix to echelon form. Suppose that $A$ reduces to an echelon form $U$ without interchange. Then there is a sequence of $p$-many replacement row operations transforming $A$ into $U$. Let $E_1, E_2, \ldots, E_p$ be the corresponding elementary matrices. Thus, 
\[(E_p\ldots E_2E_1)A = U,\] and \[A = (E_p\ldots E_2E_1)^{-1}U = (E_1^{-1}E_2^{-1}\ldots E_p^{-1})U.\] Let $L = E_1^{-1}E_2^{-1}\ldots E_p^{-1}$. Now, each elementary matrix $E_i$ is unit lower triangular. Its inverse $E_i^{-1}$ is also unit lower triangular. The products of such matrices are also unit lower triangular, which gives the $LU$ factorization.\\
\end{proof}

The proof of this theorem provides us an algorithm for computing the $LU$ factorization.
\noindent\textbf{An Algorithm for finding an $LU$ Factorization}
\begin{enumerate}[!LIST!,start=1]
\item Reduce $A$ to an echelon form $U$ by a sequence of only forward-phase row replacement operations, if possible.\\
\item Place entries in $L$ such that the same sequence of row operations reduces $L$ to $I_m$, that is, place in the $(i,j)$-position the scalar $-c$ whenever $\row i \mapsto \row i + c\row j$ was used. 
\end{enumerate}\vs

\begin{Exam}
Find an $LU$ factorization of $A = \mtx{ccccc}{2&4&10&5&9\\7&6&3&3&1\\2&6&7&1&8\\5&0&7&8&1} \pmod{11}$.\\

Since $A$ has four rows, $L$ will be $4\times 4$. Below, you will see two columns of matrices. On the left, you will see the row reduction of $A$ to an echelon form $U$ using only forward-phase replacements. On the right, you will see the matrix $L$ built brick-by-brick by placing the corresponding scalars below the diagonal.
\begin{eqnarray*}
A =\mtx{ccccc}{2&4&10&5&9\\7&6&3&3&1\\2&6&7&1&8\\5&0&7&8&1} \begin{array}{l} \mbox{}\\ \phantom{(\row 2 + 2\row 1)} \\ \mbox{} \\\mbox{} \end{array} && L = \mtx{rrrr}{1&0&0&0\\\ast &1&0&0\\\ast &\ast &1&0\\\ast &\ast &\ast &1}\\
=\mtx{ccccc}{\fbox{$2$}&4&10&5&9\\7&6&3&3&1\\2&6&7&1&8\\5&0&7&8&1}\begin{array}{l} \mbox{}\\\color{red}(\row 2 - 9\row 1) \\\color{red}(\row 3 - \row 1) \\\color{red}(\row 4 - 8\row 1)\end{array}  && L = \mtx{rrrr}{1&0&0&0\\\color{red}9&1&0&0\\\color{red}1&\ast &1&0\\\color{red}8&\ast &\ast &1}\\
\sim \mtx{ccccc}{2&4&10&5&9\\0&\fbox{$3$}&1&2&8\\0&2&8&7&10\\0&1&4&1&6}\begin{array}{l} \mbox{}\\\mbox{} \\\color{red}(\row 3 - 8\row 2) \\\color{red}(\row 4 - 4\row 2)\end{array} && L = \mtx{rrrr}{1&0&0&0\\9&1&0&0\\1&\color{red}8 &1&0\\8&\color{red}4 &\ast &1}\\
\sim \mtx{ccccc}{2&4&10&5&9\\0&3&1&2&8\\0&0&0&\fbox{$2$}&1\\0&0&0&4&7}\begin{array}{l} \mbox{}\\\mbox{} \\\mbox{} \\\color{red}(\row 4 - 2\row 3)\end{array} && L = \mtx{rrrr}{1&0&0&0\\9&1&0&0\\1&8 &1&0\\8&4 &\color{red} 2 &1}\\
\sim U=\mtx{ccccc}{2&4&10&5&9\\0&3&1&2&8\\0&0&0&2&1\\0&0&0&0&5}\begin{array}{l} \mbox{}\\\mbox{} \\\mbox{} \\ \phantom{(\row 4 - 0\row 3)}\end{array} && L = \mtx{rrrr}{1&0&0&0\\9&1&0&0\\1&8 &1&0\\8&4 & 2 &1}
\end{eqnarray*}
Thus, \[A = \mtx{ccccc}{2&4&10&5&9\\7&6&3&3&1\\2&6&7&1&8\\5&0&7&8&1} \equiv \mtx{rrrr}{1&0&0&0\\9&1&0&0\\1&8 &1&0\\8&4 & 2 &1}\mtx{rrrrr}{2&4&10&5&9\\0&3&1&2&8\\0&0&0&2&1\\0&0&0&0&5} = LU \pmod{11}.\qedhere\]
\end{Exam}\vs

\begin{multicols}{2}
Consider the system of matrix equations depicted.  Such a system might be considered, for example, if one needs to determine whether the list of vectors $\{\bb b_1, \bb b_2, \ldots, \bb b_p\}$ is in the span of the column vectors of $A$ and, if so, what coefficients for a linear combination will satisfy the equations.\columnbreak  
\[\begin{linear}
A\bb x\ &=\ & \bb b_1\\
A\bb x\ &=\ & \bb b_2\\
& \vdots &\\
A\bb x\ &=\ & \bb b_p
\end{linear}\]
\end{multicols}
\vspace{-0.15 in}\noindent
Each equation $A\bb x = \bb b_i$ is solved using the SAME row operations to compute an echelon form of $A$. 
So computationally, it is often more efficient to 
record the necessary row operations once and for all.  This is the case with past row-reductions such as \hyperref[eq:changeofbasismatrix]{$[ \B \mid \c]$} or \hyperref[eq:inversionalgorithm]{$[A \mid I_n]$}. This leads to the $LU$ factorization.\\


We have seen the convenience of solving an augmented matrix in echelon form (aka upper triangular matrix). Back substitution saves the day! Likewise, if a matrix is in lower triangular form, then it is essentially in ``upside-down'' echelon form and back substitution will lead to a quick solution of the augmented matrix. Therefore, suppose that 
\[A = LU\] is an $LU$ factorization and consider the matrix equation \[A\bb x = \bb b.\] Let $\bb y = U\bb x$. Then 
\[A\bb x = (LU)\bb x = L(U \bb x) = L\bb y = \bb b.\] Thus, solving the equation $A\bb x = \bb b$ boils down to solving the two dramatically simpler matrix equations 
\[L\bb y  = \bb b \quad\text{and}\quad U \bb x = \bb y\]

\begin{Exam}\label{LU} We saw in the previous example that 
\[A = \mtx{ccccc}{2&4&10&5&9\\7&6&3&3&1\\2&6&7&1&8\\5&0&7&8&1} \equiv  \mtx{rrrr}{1&0&0&0\\9&1&0&0\\1&8 &1&0\\8&4 & 2 &1}\mtx{rrrrr}{2&4&10&5&9\\0&3&1&2&8\\0&0&0&2&1\\0&0&0&0&5} = LU \pmod{11}.\] Using this $LU$ factorization of $A$, solve $A\bb x = \bb b$, where $\bb b = (1,2,3,4)$.\\

We first solve the matrix equation $L\bb y = \bb b$, which corresponds to the augmented matrix: 
\[\mtx{cccc|c}{\fbox{1}&0&0&0&1\\9&1&0&0&2\\1&8&1&0&3\\8&4&2&1&4} \sim \mtx{cccc|c}{1&0&0&0&1\\0&\fbox{1}&0&0&4\\0&8&1&0&2\\0&4&2&1&7} \sim \mtx{cccc|c}{1&0&0&0&1\\0&1&0&0&4\\0&0&\fbox{1}&0&3\\0&0&2&1&2}
\sim \mtx{cccc|c}{1&0&0&0&1\\0&1&0&0&4\\0&0&1&0&3\\0&0&0&1&7}.\] Thus, $\bb y = (1,4,3,7)$. 

Next, we need to solve the equation $U\bb x = \bb y$:\\
%\begin{eqnarray*}
\begin{multline*} \mtx{ccccc|c}{2&4&10&5&9&1\\0&3&1&2&8&4\\0&0&0&2&1&3\\0&0&0&0&\fbox{5}&7}\sim \mtx{ccccc|c}{2&4&10&5&9&1\\0&3&1&2&8&4\\0&0&0&2&1&3\\0&0&0&0&\fbox{1}&8}\sim \mtx{ccccc|c}{2&4&10&5&0&6\\0&3&1&2&0&6\\0&0&0&\fbox{2}&0&6\\0&0&0&0&1&8} \sim \mtx{ccccc|c}{2&4&10&5&0&6\\0&3&1&2&0&6\\0&0&0&\fbox{1}&0&3\\0&0&0&0&1&8}\\ \sim \mtx{ccccc|c}{2&4&10&0&0&2\\0&\fbox{3}&1&0&0&0\\0&0&0&1&0&3\\0&0&0&0&1&8}\sim \mtx{ccccc|c}{2&4&10&0&0&2\\0&\fbox{1}&4&0&0&0\\0&0&0&1&0&3\\0&0&0&0&1&8}\sim \mtx{ccccc|c}{\fbox{2}&0&5&0&0&2\\0&1&4&0&0&0\\0&0&0&1&0&3\\0&0&0&0&1&8}\sim \mtx{ccccc|c}{1&0&8&0&0&1\\0&1&4&0&0&0\\0&0&0&1&0&3\\0&0&0&0&1&8}
\end{multline*}
Thus, $\bb x = (1,0,0,3,8) + t(3,7,1,0,0)$ is the general solution.\\

We should mention that the above example required 27 arithmetic operations while the usual row-reduction method would have required 46 operations.
\end{Exam}\vs

To handle row interchanges, the $LU$ factorization above can be modified easily to produce an $PLU$-factorization, where $P$ is a permutation matrix. Similarly, to handle row scaling, a diagonal matrix $D$ can be introduced to given instead the $LDU$-factorization. Combining all three row operations together leads to the $PLDU$-factorization. The corresponding system of equation of each of these factorizations is still relatively easy to solve.

%%%%%%%%%%%%%%%%%% Exercises %%%%%%%%%%%%%%%%%%%
\startExercises{lu}
\noindent For Exercises \ref{exer:scalarcheckstart}-\ref{exer:scalarcheckstop}, determine if the matrix is a scalar matrix.
\begin{enumerate}[!HW!, start=1]
\begin{multicols}{4}
\item\label{exer:scalarcheckstart} $\mtx{rrr}{1&0&0\\0&1&0\\0&0&1}$ %Noah Swenson
\item $\mtx{rr}{2&0\\1&2}$ %Noah Swenson
\item $\mtx{rr}{3&0\\0&3\\0&0}$ %Noah Swenson
\item\label{exer:scalarcheckstop} $\mtx{rrr}{0&0&0\\0&0&0\\0&0&0}$ %Noah Swenson
\end{multicols}
\end{enumerate}


\noindent QUICK! For Exercises \ref{exer:diagonalmultiplystart}-\ref{exer:diagonalmultiplystop}, multiply by the diagonal matrices using the discussion following \defref{def:diagonal} in LESS THAN 10 SECONDS! 
\begin{enumerate}[!HW!, label=$\spadesuit$ \arabic*., ref=\arabic*]
\begin{multicols}{2}
\item\label{exer:diagonalmultiplystart} $\mtx{rrr}{2&0&0\\0&-1&0\\0&0&-3}\mtx{rr}{1&2\\-3&2\\1&5}$\\ 
\itemspade $\mtx{rrr}{-3&0&0\\0&5&0\\0&0&2}\mtx{rrr}{1&2&3\\0&-1&5\\2&-2&-3}\mtx{rrr}{3&0&0\\0&-1&0\\0&0&-2}$
\end{multicols}
\item\label{exer:diagonalmultiplystop} $\mtx{rrr}{1&0&0\\0&0&0\\0&0&5}\mtx{rrr}{1&2&0\\3&-1&4\\-2&1&-5}\mtx{rrr}{0&0&0\\0&3&0\\0&0&1}$
\end{enumerate}

\noindent For Exercises \ref{exer:diagonalcomputestart}-\ref{exer:diagonalcomputestop}, find $A^2$, $A^3$, $A^{-1}$, and $A^{-3}$.
\begin{enumerate}[!HW!, label=$\spadesuit$ \arabic*., ref=\arabic*]
\item\label{exer:diagonalcomputestart}\label{exer:diagonalcomputestop}  $A= \mtx{rrr}{2&0&0\\0&-3&0\\0&0&5}$. %NEW 
\end{enumerate}

\noindent For Exercises \ref{exer:permutefactorstart}-\ref{exer:permutefactorstop}, the permutation matrix $P$ will be listed first and another matrix $A$ will be listed second. Factor $P$ as a product of interchange elementary matrices (answers may vary). Compute $PA$ and explain how the rows of $A$ have been permuted. Compute $AP$ and explain how the rows of $A$ have been permuted.
\begin{enumerate}[!HW!]
\item\label{exer:permutefactorstart}\label{exer:permutefactorstop} $\mtx{rrrr}{0&0&1&0\\1&0&0&0\\0&0&0&1\\0&1&0&0},\ \mtx{rrrr}{1&5&8&10\\5&2&6&9\\8&6&3&7\\10&9&7&4}$ %Hannah Chappell
\end{enumerate}

\noindent For Exercises \ref{exer:triangularfactorstart}-\ref{exer:triangularfactorstop}, factor the following matrices as a product of elementary matrices. %NEW
\begin{enumerate}[!HW!, label=$\spadesuit$ \arabic*., ref=\arabic*]
\begin{multicols}{2}
\item\label{exer:triangularfactorstart} $\mtx{rrr}{1&0&0\\2&1&0\\3&4&1}$
\item\label{exer:triangularfactorstop} $\mtx{rrr}{3&12&-6\\0&5&45\\0&0&2}$ 
\end{multicols}
\end{enumerate}

\noindent For Exercises \ref{exer:lufactorstart}-\ref{exer:lufactorstop}, find the $LU$-factorization of the provided matrix.
\begin{enumerate}[!HW!]
\item\label{exer:lufactorstart}\label{exer:lufactorstop} $\mtx{rrrrr}{2&4&-1&5&-2\\-4&-5&3&-8&1\\2&-5&-4&1&8\\-6&0&7&-3&1}$ %Da Huo
\end{enumerate}

\noindent For Exercises \ref{exer:lusolvestart}-\ref{exer:lusolvestop}, find the $LU$-factorization of the coefficient matrix $A$ below and use this factorization to solve the linear system as in \examref{LU}. [\textbf{Hint}: Remember to NOT scale NOR interchange.]
\begin{enumerate}[!HW!, label=$\spadesuit$ \arabic*., ref=\arabic*]
\begin{multicols}{2}
\item\label{exer:lusolvestart} $\mtx{rr}{2&-5\\-4&13}\vr{x_1\\x_2} = \vr{-2\\-2}$\\ 
\itemspade $\mtx{rr}{2&-2\\-3&6}\vr{x_1\\x_2} = \vr{4\\9}$ 
\end{multicols}
\begin{multicols}{2}
\itemspade $\mtx{rrr}{2&2&-2\\-4&-3&4\\-1&-2&4}\vr{x_1\\x_2\\x_3} = \vr{-4\\-2\\6}$
\item\label{exer:lusolvestop} $\mtx{rrr}{2&1&2\\6&6&0\\4&5&3}\vr{x_1\\x_2\\x_3} \equiv \vr{1\\2\\3} \pmod 7$
\end{multicols}
\end{enumerate}

\begin{enumerate}[!HW!]
\item Show that if $A$ is a lower triangular matrix then $A^\top $ is upper triangular.\\ %NEW
\end{enumerate}

%%%%%%%%%%%%%%%%%%% Footnotes %%%%%%%%%%%%%%%%%%%
 \mbox{}\vfill
 \footnotetext[2]{I suppose it could be called a \emph{chandelier form}.}
\pagebreak